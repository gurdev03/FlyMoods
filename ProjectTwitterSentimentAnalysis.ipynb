{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2888f7ef",
   "metadata": {},
   "source": [
    "# Twitter US Airline Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa711f2",
   "metadata": {},
   "source": [
    "Given Twitter US Airline Sentiment Dataset, which contains data for over 14000 tweets,\n",
    "\n",
    "your task:\n",
    "\n",
    "  is to predict the sentiment of the tweet i.e. positive, negative or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834686c",
   "metadata": {},
   "source": [
    "You are given:\n",
    "1. A Training dataset csv file with X train and Y train data\n",
    "2. A X test File and you have to predict and submit predictions for this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c36c3ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0122374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98330b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "033b14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b19620df",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv(\"C:/Users/hp/Downloads/twitter_x_test.csv\")\n",
    "testing_df = pd.read_csv(\"C:/Users/hp/Downloads/twitter_x_y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86626baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae560f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>569682010270101504</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zsalim03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir In car gng to DFW. Pulled over 1h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 18:15:50 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>569608307184242688</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sa_craig</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir after all, the plane didn’t land ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 13:22:57 -0800</td>\n",
       "      <td>College Station, TX</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>567879304593408001</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DanaChristos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>@SouthwestAir can't believe how many paying cu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 18:52:31 -0800</td>\n",
       "      <td>CT</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    airline airline_sentiment_gold          name  \\\n",
       "0  569682010270101504   American                    NaN      zsalim03   \n",
       "1  569608307184242688   American                    NaN      sa_craig   \n",
       "2  567879304593408001  Southwest                    NaN  DanaChristos   \n",
       "\n",
       "  negativereason_gold  retweet_count  \\\n",
       "0                 NaN              0   \n",
       "1                 NaN              0   \n",
       "2                 NaN              1   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  @AmericanAir In car gng to DFW. Pulled over 1h...         NaN   \n",
       "1  @AmericanAir after all, the plane didn’t land ...         NaN   \n",
       "2  @SouthwestAir can't believe how many paying cu...         NaN   \n",
       "\n",
       "               tweet_created       tweet_location               user_timezone  \n",
       "0  2015-02-22 18:15:50 -0800                Texas  Central Time (US & Canada)  \n",
       "1  2015-02-22 13:22:57 -0800  College Station, TX  Central Time (US & Canada)  \n",
       "2  2015-02-17 18:52:31 -0800                   CT  Eastern Time (US & Canada)  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76add3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>567900433542488064</td>\n",
       "      <td>negative</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ColeyGirouard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir I am scheduled for the morning, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 20:16:29 -0800</td>\n",
       "      <td>Washington D.C.</td>\n",
       "      <td>Atlantic Time (Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>569989168903819264</td>\n",
       "      <td>positive</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WalterFaddoul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir seeing your workers time in and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 14:36:22 -0800</td>\n",
       "      <td>Indianapolis, Indiana; USA</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>568089179520954368</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LocalKyle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united Flew ORD to Miami and back and  had gr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-18 08:46:29 -0800</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment    airline airline_sentiment_gold  \\\n",
       "0  567900433542488064          negative  Southwest                    NaN   \n",
       "1  569989168903819264          positive  Southwest                    NaN   \n",
       "2  568089179520954368          positive     United                    NaN   \n",
       "\n",
       "            name negativereason_gold  retweet_count  \\\n",
       "0  ColeyGirouard                 NaN              0   \n",
       "1  WalterFaddoul                 NaN              0   \n",
       "2      LocalKyle                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  @SouthwestAir I am scheduled for the morning, ...         NaN   \n",
       "1  @SouthwestAir seeing your workers time in and ...         NaN   \n",
       "2  @united Flew ORD to Miami and back and  had gr...         NaN   \n",
       "\n",
       "               tweet_created              tweet_location  \\\n",
       "0  2015-02-17 20:16:29 -0800             Washington D.C.   \n",
       "1  2015-02-23 14:36:22 -0800  Indianapolis, Indiana; USA   \n",
       "2  2015-02-18 08:46:29 -0800                    Illinois   \n",
       "\n",
       "                user_timezone  \n",
       "0      Atlantic Time (Canada)  \n",
       "1  Central Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2346e159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e12782a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement nltk.tokenize (from versions: none)\n",
      "ERROR: No matching distribution found for nltk.tokenize\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "062a11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c39f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data = training_df['text']\n",
    "y_train_data = training_df['airline']\n",
    "x_test_data = testing_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "541cd00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5cc8e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y_train_documents = []\n",
    "for i in range(len(x_train_data)):\n",
    "    document = word_tokenize(x_train_data[i])\n",
    "    category = y_train_data[i]\n",
    "    x_y_train_documents.append((document, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9eef08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_documents = []\n",
    "for i in range(len(x_test_data)):\n",
    "    document = word_tokenize(x_test_data[i])\n",
    "    x_test_documents.append((document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "232b51cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['@', 'AmericanAir', 'In', 'car', 'gng', 'to', 'DFW', '.', 'Pulled', 'over', '1hr', 'ago', '-', 'very', 'icy', 'roads', '.', 'On-hold', 'with', 'AA', 'since', '1hr', '.', 'Ca', \"n't\", 'reach', 'arpt', 'for', 'AA2450', '.', 'Wat', '2', 'do', '?'], 'American')\n",
      "----------------------------------------------------------------------\n",
      "['@', 'SouthwestAir', 'I', 'am', 'scheduled', 'for', 'the', 'morning', ',', '2', 'days', 'after', 'the', 'fact', ',', 'yes', '..', 'not', 'sure', 'why', 'my', 'evening', 'flight', 'was', 'the', 'only', 'one', 'Cancelled', 'Flightled']\n"
     ]
    }
   ],
   "source": [
    "print(x_y_train_documents[0])\n",
    "print('----------------------------------------------------------------------')\n",
    "print(x_test_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ec592cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "316277fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"v\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "033bdaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e9958577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2e352e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('better', 'RBR')]\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag([\"better\"]))\n",
    "print(get_simple_pos(\"R\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ee0fae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1236c176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "punctuations = list(string.punctuation)\n",
    "stop = stop + punctuations\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b04f4e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(words):     # array -> words\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stop:\n",
    "            # lemmatize word\n",
    "            pos = pos_tag([w])\n",
    "            clean_word = lemmatizer.lemmatize(w, get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word)\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d9537f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.59828352928162\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "training_documents = [(clean_review(document),category) for document,category in x_y_train_documents]\n",
    "testing_documents = [(clean_review(document)) for document in x_test_documents]\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f3100833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['AmericanAir', 'car', 'gng', 'DFW', 'Pulled', '1hr', 'ago', 'icy', 'road', 'On-hold', 'AA', 'since', '1hr', 'Ca', \"n't\", 'reach', 'arpt', 'AA2450', 'Wat', '2'], 'American'), (['AmericanAir', 'plane', '’', 'land', 'identical', 'bad', 'condition', 'GRK', 'according', 'METARs'], 'American')]\n"
     ]
    }
   ],
   "source": [
    "print(training_documents[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "feb5e816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SouthwestAir', 'scheduled', 'morning', '2', 'day', 'fact', 'yes', '..', 'sure', 'evening', 'flight', 'one', 'Cancelled', 'Flightled'], ['SouthwestAir', 'seeing', 'worker', 'time', 'time', 'going', 'beyond', 'love', 'flying', 'guy', 'Thank']]\n"
     ]
    }
   ],
   "source": [
    "print(testing_documents[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6e81517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create feature\n",
    "all_words = []\n",
    "for doc in training_documents:\n",
    "    all_words += doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9efe5e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40179"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f1ba10db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flight', 965),\n",
       " ('united', 900),\n",
       " ('USAirways', 762),\n",
       " ('AmericanAir', 717),\n",
       " ('SouthwestAir', 606)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = nltk.FreqDist(all_words)\n",
    "common = freq.most_common(5000)\n",
    "common[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a1cb75a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [i[0] for i in common]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "23f50482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need cover list of word into one str\n",
    "x_train_data = [\" \".join(document) for document, category in training_documents]\n",
    "y_train_data = [category for document, category in training_documents]\n",
    "y_train_data = np.array(y_train_data)\n",
    "x_test_data = [\" \".join(document) for document in testing_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e93fdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmericanAir car gng DFW Pulled 1hr ago icy road On-hold AA since 1hr Ca n't reach arpt AA2450 Wat 2\n"
     ]
    }
   ],
   "source": [
    "print(x_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "32dbe1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SouthwestAir scheduled morning 2 day fact yes .. sure evening flight one Cancelled Flightled\n"
     ]
    }
   ],
   "source": [
    "print(x_test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b78b4150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn-features in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sklearn-features) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sklearn-features) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sklearn-features) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->sklearn-features) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from pandas->sklearn-features) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->sklearn-features) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->sklearn-features) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sklearn-features) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sklearn-features) (3.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->sklearn-features) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4df61ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2cba2707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "count_vec = CountVectorizer(max_features = 10000, ngram_range = (1,2), max_df = 0.7)\n",
    "x_train_features = count_vec.fit_transform(x_train_data)\n",
    "x_test_features = count_vec.transform(x_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f9511c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3660, 10000), (3660,), (10980, 10000))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features.shape, y_train_data.shape, x_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d921d7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d9a5ba3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['American', 'American', 'Southwest', ..., 'Virgin America',\n",
       "       'American', 'United'], dtype='<U14')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "20ae2b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'count_vec.get_feature_names()'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''count_vec.get_feature_names()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9234926",
   "metadata": {},
   "source": [
    "## Train Support Vector Machine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "285863c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(x_train_features, y_train_data)\n",
    "svc.score(x_train_features, y_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d528b25",
   "metadata": {},
   "source": [
    "## Train Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "97010590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train_features, y_train_data)\n",
    "rfc.score(x_train_features, y_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be343149",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b89097cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Southwest', 'Southwest', 'United', ..., 'US Airways',\n",
       "       'US Airways', 'United'], dtype='<U14')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rfc.predict(x_test_features)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec1d91aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Southwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Southwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Southwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0  Southwest\n",
       "1  Southwest\n",
       "2     United\n",
       "3  Southwest\n",
       "4     United"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "y_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa1105f",
   "metadata": {},
   "source": [
    "2. Submit a csv file with only predictions for X test data. File should not have any headers and should only have one column i.e. predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "514cb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df.to_csv(\"Y_Predicted.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "34f82c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.65254664421082\n"
     ]
    }
   ],
   "source": [
    "ending = time.time()\n",
    "print( ending - starting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
